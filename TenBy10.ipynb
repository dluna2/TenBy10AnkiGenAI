{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-zroN3yVlm9"
      },
      "source": [
        "#Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r3McmEiS5qE",
        "outputId": "e0c43f45-8008-4369-f288-cc6b87c0f51b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cerebras-cloud-sdk in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.59.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: gradio in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (5.49.1)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cerebras-cloud-sdk) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cerebras-cloud-sdk) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cerebras-cloud-sdk) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cerebras-cloud-sdk) (2.11.10)\n",
            "Requirement already satisfied: sniffio in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cerebras-cloud-sdk) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cerebras-cloud-sdk) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->cerebras-cloud-sdk) (3.11)\n",
            "Requirement already satisfied: certifi in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->cerebras-cloud-sdk) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->cerebras-cloud-sdk) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->cerebras-cloud-sdk) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->cerebras-cloud-sdk) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->cerebras-cloud-sdk) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->cerebras-cloud-sdk) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yuna\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: audioop-lts<1.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (0.2.2)\n",
            "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (0.121.1)\n",
            "Requirement already satisfied: ffmpy in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (0.6.4)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (1.1.2)\n",
            "Requirement already satisfied: jinja2<4.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in c:\\users\\yuna\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydub in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (0.14.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio-client==1.13.3->gradio) (2025.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: shellingham in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: typer-slim in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (0.20.0)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting regex (from sacrebleu)\n",
            "  Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
            "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\yuna\\appdata\\roaming\\python\\python313\\site-packages (from sacrebleu) (0.4.6)\n",
            "Collecting lxml (from sacrebleu)\n",
            "  Downloading lxml-6.0.2-cp313-cp313-win_amd64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\yuna\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\yuna\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Collecting pywin32>=226 (from portalocker->sacrebleu)\n",
            "  Using cached pywin32-311-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Downloading lxml-6.0.2-cp313-cp313-win_amd64.whl (4.0 MB)\n",
            "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
            "   ------------------ --------------------- 1.8/4.0 MB 8.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 3.7/4.0 MB 8.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 4.0/4.0 MB 7.9 MB/s  0:00:00\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Using cached pywin32-311-cp313-cp313-win_amd64.whl (9.5 MB)\n",
            "Downloading regex-2025.11.3-cp313-cp313-win_amd64.whl (277 kB)\n",
            "Installing collected packages: pywin32, tabulate, regex, portalocker, lxml, sacrebleu\n",
            "\n",
            "   ---------------------------------------- 0/6 [pywin32]\n",
            "   ---------------------------------------- 0/6 [pywin32]\n",
            "   ---------------------------------------- 0/6 [pywin32]\n",
            "   ---------------------------------------- 0/6 [pywin32]\n",
            "   ---------------------------------------- 0/6 [pywin32]\n",
            "   ---------------------------------------- 0/6 [pywin32]\n",
            "   ---------------------------------------- 0/6 [pywin32]\n",
            "   ---------------------------------------- 0/6 [pywin32]\n",
            "   ------------- -------------------------- 2/6 [regex]\n",
            "   -------------------------- ------------- 4/6 [lxml]\n",
            "   --------------------------------- ------ 5/6 [sacrebleu]\n",
            "   ---------------------------------------- 6/6 [sacrebleu]\n",
            "\n",
            "Successfully installed lxml-6.0.2 portalocker-3.2.0 pywin32-311 regex-2025.11.3 sacrebleu-2.5.1 tabulate-0.9.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install cerebras-cloud-sdk pandas gradio sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNtUCMihm53a",
        "outputId": "bdcde83a-d54e-4563-fd82-2ec3046f5461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "     ---------------------------------------- 0.0/803.2 kB ? eta -:--:--\n",
            "     ---------------------------------------- 803.2/803.2 kB 9.7 MB/s  0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting more-itertools (from openai-whisper)\n",
            "  Downloading more_itertools-10.8.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting numba (from openai-whisper)\n",
            "  Downloading numba-0.63.1-cp313-cp313-win_amd64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai-whisper) (2.3.4)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.12.0-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: torch in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai-whisper) (2.9.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai-whisper) (4.67.1)\n",
            "Collecting requests<3,>=2.27 (from gTTS)\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting click<8.2,>=7.1 (from gTTS)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\yuna\\appdata\\roaming\\python\\python313\\site-packages (from click<8.2,>=7.1->gTTS) (0.4.6)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.27->gTTS)\n",
            "  Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl.metadata (38 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->gTTS) (3.11)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.27->gTTS)\n",
            "  Downloading urllib3-2.6.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->gTTS) (2025.10.5)\n",
            "Collecting llvmlite<0.47,>=0.46.0dev0 (from numba->openai-whisper)\n",
            "  Downloading llvmlite-0.46.0-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->openai-whisper) (2025.10.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch->openai-whisper) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yuna\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl (107 kB)\n",
            "Downloading urllib3-2.6.1-py3-none-any.whl (131 kB)\n",
            "Downloading more_itertools-10.8.0-py3-none-any.whl (69 kB)\n",
            "Downloading numba-0.63.1-cp313-cp313-win_amd64.whl (2.8 MB)\n",
            "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.8/2.8 MB 21.3 MB/s  0:00:00\n",
            "Downloading llvmlite-0.46.0-cp313-cp313-win_amd64.whl (38.1 MB)\n",
            "   ---------------------------------------- 0.0/38.1 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 3.9/38.1 MB 20.8 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 7.6/38.1 MB 19.1 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 12.6/38.1 MB 20.0 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 16.3/38.1 MB 19.4 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 20.4/38.1 MB 19.2 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 23.3/38.1 MB 18.7 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 26.2/38.1 MB 17.7 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 30.7/38.1 MB 18.3 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 35.1/38.1 MB 18.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  38.0/38.1 MB 18.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 38.1/38.1 MB 17.7 MB/s  0:00:02\n",
            "Downloading tiktoken-0.12.0-cp313-cp313-win_amd64.whl (879 kB)\n",
            "   ---------------------------------------- 0.0/879.1 kB ? eta -:--:--\n",
            "   ---------------------------------------- 879.1/879.1 kB 17.1 MB/s  0:00:00\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml): started\n",
            "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=804013 sha256=229434efcf5ea78fa6328faabe6859bfc3f511d2e8b1a2fe899e0982caa2e8a9\n",
            "  Stored in directory: c:\\users\\yuna\\appdata\\local\\pip\\cache\\wheels\\ca\\58\\d5\\fb4539ad74c3ca81eb40f7eda020ac77d080b33ad57449d485\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: urllib3, more-itertools, llvmlite, click, charset_normalizer, requests, numba, tiktoken, gTTS, openai-whisper\n",
            "\n",
            "   ---- -----------------------------------  1/10 [more-itertools]\n",
            "   -------- -------------------------------  2/10 [llvmlite]\n",
            "   -------- -------------------------------  2/10 [llvmlite]\n",
            "   -------- -------------------------------  2/10 [llvmlite]\n",
            "  Attempting uninstall: click\n",
            "   -------- -------------------------------  2/10 [llvmlite]\n",
            "    Found existing installation: click 8.3.0\n",
            "   -------- -------------------------------  2/10 [llvmlite]\n",
            "    Uninstalling click-8.3.0:\n",
            "   -------- -------------------------------  2/10 [llvmlite]\n",
            "      Successfully uninstalled click-8.3.0\n",
            "   -------- -------------------------------  2/10 [llvmlite]\n",
            "   ------------ ---------------------------  3/10 [click]\n",
            "   -------------------- -------------------  5/10 [requests]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ------------------------ ---------------  6/10 [numba]\n",
            "   ---------------------------- -----------  7/10 [tiktoken]\n",
            "   ---------------------------------------- 10/10 [openai-whisper]\n",
            "\n",
            "Successfully installed charset_normalizer-3.4.4 click-8.1.8 gTTS-2.5.4 llvmlite-0.46.0 more-itertools-10.8.0 numba-0.63.1 openai-whisper-20250625 requests-2.32.5 tiktoken-0.12.0 urllib3-2.6.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install openai-whisper gTTS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx7vrY5SN3gR"
      },
      "source": [
        "#Tenby10 Backend Experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cDFEKb0eS5n8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Yuna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from cerebras.cloud.sdk import Cerebras\n",
        "import pandas as pd\n",
        "import json\n",
        "import gradio as gr\n",
        "import sacrebleu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UdBpXQXiG2ji"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from cerebras.cloud.sdk import Cerebras\n",
        "import pandas as pd\n",
        "import json\n",
        "import gradio as gr\n",
        "import sacrebleu\n",
        "\n",
        "\n",
        "client = Cerebras(api_key=os.environ[\"CEREBRAS_API_KEY_DANI\"])\n",
        "\n",
        "def generate_vocab_flashcards(vocab_list, target_language=\"Spanish\"):\n",
        "    results = []\n",
        "    has_reading = target_language.lower() in [\"japanese\", \"chinese\"]\n",
        "\n",
        "    for word in vocab_list:\n",
        "        # Prompt asks for reading if Japanese or Chinese\n",
        "        if has_reading:\n",
        "            prompt = f\"\"\"\n",
        "            You are a multilingual language learning assistant.\n",
        "            For the term \\\"{word}\\\", translate it into {target_language}, give the pronunciation (reading,\n",
        "            such as pinyin for Chinese or furigana for Japanese in [brackets] for the reading and example sentence),\n",
        "            and provide one natural example sentence in\n",
        "            the input and target language.\n",
        "            Respond ONLY in valid JSON with this exact structure:\n",
        "            {{\n",
        "                \"term\": \\\"{word}\\\",\n",
        "                \"translation\": \"...\",\n",
        "                \"reading\": \"...\",\n",
        "                \"example_sentence\": \"...\"\n",
        "            }}\n",
        "            \"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"\n",
        "            You are a multilingual language learning assistant.\n",
        "            For the term \\\"{word}\\\", translate it into {target_language}, and give one natural example\n",
        "            sentence in the input language and target language.\n",
        "            Respond ONLY in valid JSON with this exact structure:\n",
        "            {{\n",
        "                \"term\": \\\"{word}\\\",\n",
        "                \"translation\": \"...\",\n",
        "                \"example_sentence\": \"...\"\n",
        "            }}\n",
        "            \"\"\"\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"qwen-3-235b-a22b-instruct-2507\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful multilingual assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_completion_tokens=500,\n",
        "                temperature=0.7,\n",
        "                top_p=0.5,\n",
        "                stream=False\n",
        "            )\n",
        "            content = completion.choices[0].message.content.strip()\n",
        "            try:\n",
        "                data = json.loads(content)\n",
        "            except:\n",
        "                # Fallback: handle JSON errors\n",
        "                if has_reading:\n",
        "                    data = {\n",
        "                        \"term\": word,\n",
        "                        \"translation\": content.split(\"\\n\")[0] if \"\\n\" in content else content,\n",
        "                        \"reading\": \"\",\n",
        "                        \"example_sentence\": \"\"\n",
        "                    }\n",
        "                else:\n",
        "                    data = {\n",
        "                        \"term\": word,\n",
        "                        \"translation\": content.split(\"\\n\")[0] if \"\\n\" in content else content,\n",
        "                        \"example_sentence\": \"\"\n",
        "                    }\n",
        "            if has_reading:\n",
        "                results.append({\n",
        "                    \"Grammar/Vocab\": data.get(\"term\", word),\n",
        "                    \"Reading\": data.get(\"reading\", \"\"),\n",
        "                    \"Translation\": data.get(\"translation\", \"\"),\n",
        "                    \"Example Sentence\": data.get(\"example_sentence\", \"\")\n",
        "                })\n",
        "            else:\n",
        "                results.append({\n",
        "                    \"Grammar/Vocab\": data.get(\"term\", word),\n",
        "                    \"Translation\": data.get(\"translation\", \"\"),\n",
        "                    \"Example Sentence\": data.get(\"example_sentence\", \"\")\n",
        "                })\n",
        "        except Exception as e:\n",
        "            if has_reading:\n",
        "                results.append({\n",
        "                    \"Grammar/Vocab\": word,\n",
        "                    \"Reading\": \"\",\n",
        "                    \"Translation\": f\"Error: {e}\",\n",
        "                    \"Example Sentence\": \"\"\n",
        "                })\n",
        "            else:\n",
        "                results.append({\n",
        "                    \"Grammar/Vocab\": word,\n",
        "                    \"Translation\": f\"Error: {e}\",\n",
        "                    \"Example Sentence\": \"\"\n",
        "                })\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Nu-eKmCLcgQb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import json\n",
        "\n",
        "GRAMMAR_CSV_PATH = \"grammar_flashcards.csv\"\n",
        "GRAMMAR_COLUMNS = [\n",
        "    \"grammar point\",\n",
        "    \"grammar point translation\",\n",
        "    \"conjugation rules\",\n",
        "    \"example sentence\",\n",
        "    \"example sentence translation\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T1y9JDLdcipc"
      },
      "outputs": [],
      "source": [
        "def append_grammar_row(data):\n",
        "    \"\"\"\n",
        "    Append a single grammar flashcard row to grammar_flashcards.csv.\n",
        "    `data` keys come from the model JSON.\n",
        "    \"\"\"\n",
        "    row = {\n",
        "        \"grammar point\": data.get(\"grammar_point\", \"\"),\n",
        "        \"grammar point translation\": data.get(\"grammar_point_translation\", \"\"),\n",
        "        \"conjugation rules\": data.get(\"conjugation_rules\", \"\"),\n",
        "        \"example sentence\": data.get(\"example_sentence\", \"\"),\n",
        "        \"example sentence translation\": data.get(\"example_sentence_translation\", \"\"),\n",
        "    }\n",
        "\n",
        "    file_exists = os.path.isfile(GRAMMAR_CSV_PATH)\n",
        "\n",
        "    with open(GRAMMAR_CSV_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=GRAMMAR_COLUMNS)\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHDJ_bkvcMnx",
        "outputId": "a0c7b839-9469-4613-976e-6d66073d0a23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [00:30<00:00, 15.6MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from gtts import gTTS\n",
        "import tempfile\n",
        "import whisper\n",
        "\n",
        "def generate_grammar_flashcard_and_explanation(\n",
        "    user_message,\n",
        "    target_language=\"Spanish\",\n",
        "    native_language=\"English\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Given a learner's question, produce:\n",
        "      - a natural explanation (string) to show in chat\n",
        "      - a structured grammar flashcard dict to save in CSV\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a grammar tutor helping a learner whose native language is {native_language}.\n",
        "    The target language is {target_language}.\n",
        "\n",
        "    The learner asked:\n",
        "    \"{user_message}\"\n",
        "\n",
        "    Your tasks:\n",
        "\n",
        "    1. Identify ONE main grammar point in the target language that best matches this question.\n",
        "       Examples:\n",
        "         - For Spanish: \"Present tense of 'tener' (to have)\".\n",
        "         - For Japanese 'to have', discuss nuance between いる, ある, 飼っている, 持っている.\n",
        "    2. Provide the translation of that grammar point into {native_language}.\n",
        "    3. Describe the conjugation rules or usage patterns clearly.\n",
        "       - For verbs like Spanish \"tener\", list the key forms (tengo, tienes, tiene, etc.).\n",
        "       - For Japanese \"to have\", explain when to use いる, ある, 飼っている, 持っている and why.\n",
        "    4. Provide ONE good example sentence in the target language.\n",
        "    5. Provide the {native_language} translation of that example.\n",
        "\n",
        "    You must respond in the following exact format:\n",
        "\n",
        "    [EXPLANATION]\n",
        "    (Write a friendly explanation for the learner. Use both {target_language} and short notes in {native_language}.)\n",
        "\n",
        "    [FLASHCARD_JSON]\n",
        "    {{\n",
        "      \"grammar_point\": \"...\",\n",
        "      \"grammar_point_translation\": \"...\",\n",
        "      \"conjugation_rules\": \"...\",\n",
        "      \"example_sentence\": \"...\",\n",
        "      \"example_sentence_translation\": \"...\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"qwen-3-235b-a22b-instruct-2507\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a patient grammar tutor.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        max_completion_tokens=700,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    content = completion.choices[0].message.content.strip()\n",
        "\n",
        "    # Default fallbacks\n",
        "    explanation = content\n",
        "    flashcard_data = None\n",
        "\n",
        "    # Try to split [EXPLANATION] and [FLASHCARD_JSON]\n",
        "    if \"[FLASHCARD_JSON]\" in content:\n",
        "        parts = content.split(\"[FLASHCARD_JSON]\", 1)\n",
        "        explanation_part = parts[0]\n",
        "        json_part = parts[1]\n",
        "\n",
        "        # Clean explanation\n",
        "        explanation = explanation_part.replace(\"[EXPLANATION]\", \"\").strip()\n",
        "\n",
        "        # Try to parse JSON\n",
        "        json_str = json_part.strip()\n",
        "        try:\n",
        "            flashcard_data = json.loads(json_str)\n",
        "        except Exception:\n",
        "            flashcard_data = None\n",
        "\n",
        "    return explanation, flashcard_data\n",
        "\n",
        "def conversation_partner_chat(\n",
        "    user_text,\n",
        "    history,\n",
        "    target_language=\"Spanish\",\n",
        "    native_language=\"English\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Simple text conversation partner:\n",
        "      - Speaks mostly in target_language\n",
        "      - Gently corrects mistakes\n",
        "      - Optionally gives very short notes in native_language\n",
        "    \"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"\n",
        "            You are a friendly conversation partner helping a learner practice {target_language}.\n",
        "            - Reply primarily in {target_language}.\n",
        "            - Keep replies short and conversational (1–3 sentences).\n",
        "            - If the learner makes a clear mistake, correct it naturally.\n",
        "            - Optionally add a very short note in {native_language} in parentheses when a grammar point is important.\n",
        "            \"\"\",\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Convert Gradio style history [(user, bot), ...] into chat messages\n",
        "    for user_msg, bot_msg in history:\n",
        "        if user_msg:\n",
        "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        if bot_msg:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"qwen-3-235b-a22b-instruct-2507\",\n",
        "        messages=messages,\n",
        "        max_completion_tokens=200,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    reply = completion.choices[0].message.content.strip()\n",
        "    history.append((user_text, reply))\n",
        "    return history\n",
        "\n",
        "\n",
        "import os\n",
        "from gtts import gTTS\n",
        "import tempfile\n",
        "import whisper\n",
        "\n",
        "# Load Whisper ASR model once\n",
        "# \"small\" is fast and accurate;\n",
        "whisper_model = whisper.load_model(\"small\")\n",
        "\n",
        "\n",
        "def transcribe_audio(audio_path, source_language=\"auto\"):\n",
        "    \"\"\"\n",
        "    Fully implemented speech-to-text using OpenAI Whisper.\n",
        "\n",
        "    audio_path: file path passed by gr.Audio(type=\"filepath\")\n",
        "    source_language: \"auto\" or language code (e.g. \"en\", \"es\", \"ja\")\n",
        "\n",
        "    Returns transcribed text.\n",
        "    \"\"\"\n",
        "    if audio_path is None or not os.path.exists(audio_path):\n",
        "        return \"(No audio detected)\"\n",
        "\n",
        "    # Run Whisper transcription\n",
        "    result = whisper_model.transcribe(audio_path, language=None if source_language == \"auto\" else source_language)\n",
        "    text = result.get(\"text\", \"\").strip()\n",
        "\n",
        "    if not text:\n",
        "        return \"(Unable to transcribe audio)\"\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def synthesize_speech(text, target_language=\"Spanish\"):\n",
        "    \"\"\"\n",
        "    Fully implemented TTS using gTTS.\n",
        "    Creates an MP3 file and returns its path.\n",
        "    \"\"\"\n",
        "    if not text or text.strip() == \"\":\n",
        "        return None\n",
        "\n",
        "    # Map human language names -> gTTS language codes\n",
        "    lang_map = {\n",
        "        \"English\": \"en\",\n",
        "        \"Spanish\": \"es\",\n",
        "        \"French\": \"fr\",\n",
        "        \"Japanese\": \"ja\",\n",
        "        \"Chinese\": \"zh-CN\",\n",
        "    }\n",
        "\n",
        "    lang_code = lang_map.get(target_language, \"en\")  # default English\n",
        "\n",
        "    # Create a temporary audio file\n",
        "    tmp_dir = tempfile.gettempdir()\n",
        "    out_path = os.path.join(tmp_dir, \"tts_reply.mp3\")\n",
        "\n",
        "    try:\n",
        "        tts = gTTS(text=text, lang=lang_code)\n",
        "        tts.save(out_path)\n",
        "    except Exception as e:\n",
        "        print(\"TTS error:\", e)\n",
        "        return None\n",
        "\n",
        "    return out_path\n",
        "\n",
        "\n",
        "def conversation_partner_voice_step(\n",
        "    audio_path,\n",
        "    history,\n",
        "    target_language=\"Spanish\",\n",
        "    native_language=\"English\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Voice pipeline:\n",
        "      1. STT: audio -> user_text\n",
        "      2. LLM: conversation partner reply\n",
        "      3. TTS: reply_text -> reply_audio\n",
        "\n",
        "    Returns: updated history, reply_audio_path\n",
        "    \"\"\"\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    if audio_path is None:\n",
        "        # No audio recorded\n",
        "        return history, None\n",
        "\n",
        "    # 1) Audio -> text\n",
        "    user_text = transcribe_audio(audio_path, source_language=native_language)\n",
        "\n",
        "    # 2) Text conversation partner\n",
        "    history = conversation_partner_chat(\n",
        "        user_text=user_text,\n",
        "        history=history,\n",
        "        target_language=target_language,\n",
        "        native_language=native_language,\n",
        "    )\n",
        "\n",
        "    # Last assistant reply\n",
        "    _, bot_reply = history[-1]\n",
        "\n",
        "    # 3) Text -> audio\n",
        "    reply_audio_path = synthesize_speech(\n",
        "        bot_reply,\n",
        "        target_language=target_language,\n",
        "    )\n",
        "\n",
        "    return history, reply_audio_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cvy75c8IqD0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Dq8um_I8Ipzi"
      },
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "\n",
        "def evaluate_translation_bleu(vocab_list, target_language, reference_translations):\n",
        "    \"\"\"\n",
        "    vocab_list: list of source terms (strings)\n",
        "    target_language: e.g. \"Spanish\"\n",
        "    reference_translations: list of gold translations, same length/order as vocab_list\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Generate model translations\n",
        "    df = generate_vocab_flashcards(vocab_list, target_language)\n",
        "\n",
        "    # 2. Extract only term translations (Translation)\n",
        "    system_translations = df[\"Translation\"].tolist()\n",
        "\n",
        "    # 3. Standard BLEU (default max_ngram_order=4)\n",
        "    bleu = sacrebleu.corpus_bleu(system_translations, [reference_translations])\n",
        "\n",
        "    # 4. BLEU-1 (unigrams only)\n",
        "    bleu1_metric = sacrebleu.metrics.BLEU(max_ngram_order=1)\n",
        "    bleu1 = bleu1_metric.corpus_score(system_translations, [reference_translations])\n",
        "\n",
        "    return {\n",
        "        \"bleu\": bleu.score,  #may delete later\n",
        "        \"bleu_1\": bleu1.score,\n",
        "        \"details\": bleu,\n",
        "        \"df\": df,\n",
        "        \"system_translations\": system_translations,\n",
        "        \"reference_translations\": reference_translations,\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4DJcjoMIpnQ",
        "outputId": "d1f9e5cf-fdb9-4774-932e-ee844636ed1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU-1 Accuracy: 77.77777777777777\n",
            "SRC: 组织\n",
            "HYP: organization\n",
            "REF: the organization\n",
            "-----\n",
            "SRC: 动物\n",
            "HYP: animal\n",
            "REF: the animal\n",
            "-----\n",
            "SRC: 这变得有点荒谬了。\n",
            "HYP: This is getting a bit ridiculous.\n",
            "REF: This is getting ridiculous.\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "# Example for Spanish -> English\n",
        "'''\n",
        "vocab_list = [\n",
        "    \"la organización\",\n",
        "    \"el animal\",\n",
        "    \"Esto se está volviendo ridículo.\"\n",
        "]\n",
        "\n",
        "reference_translations = [\n",
        "    \"the organization\",\n",
        "    \"the animal\",\n",
        "    \"This is getting stupid.\"\n",
        "]\n",
        "'''\n",
        "\n",
        "'''\n",
        "vocab_list = [\n",
        "    \"l'organisation\",\n",
        "    \"l'animal\",\n",
        "    \"Ça devient stupide.\"\n",
        "]\n",
        "\n",
        "reference_translations = [\n",
        "    \"the organization\",\n",
        "    \"the animal\",\n",
        "    \"It's becoming ridiculous.\"\n",
        "]\n",
        "'''\n",
        "\n",
        "# Example for Chinese -> English\n",
        "vocab_list = [\n",
        "    \"组织\",\n",
        "    \"动物\",\n",
        "    \"这变得有点荒谬了。\"\n",
        "]\n",
        "\n",
        "reference_translations = [\n",
        "    \"the organization\",\n",
        "    \"the animal\",\n",
        "    \"This is getting ridiculous.\"\n",
        "]\n",
        "\n",
        "\n",
        "results = evaluate_translation_bleu(vocab_list, \"English\", reference_translations)\n",
        "\n",
        "print(\"BLEU-1 Accuracy:\", results[\"bleu_1\"])\n",
        "\n",
        "for src, hyp, ref in zip(\n",
        "    vocab_list,\n",
        "    results[\"system_translations\"],\n",
        "    results[\"reference_translations\"]\n",
        "):\n",
        "    print(f\"SRC: {src}\")\n",
        "    print(f\"HYP: {hyp}\")\n",
        "    print(f\"REF: {ref}\")\n",
        "    print(\"-----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hfUjxn-Iqxk"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "T2L4dEqVS5lZ",
        "outputId": "57367b97-9fad-49f3-f4fb-a6f4f189b649"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Yuna\\AppData\\Local\\Temp\\ipykernel_22596\\1202747799.py:100: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  grammar_chat = gr.Chatbot(label=\"Ask about grammar\")\n",
            "C:\\Users\\Yuna\\AppData\\Local\\Temp\\ipykernel_22596\\1202747799.py:161: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  voice_chat = gr.Chatbot(label=\"Conversation transcript\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def run_flashcard_generator(text, language):\n",
        "    vocab_list = [w.strip() for w in text.split(\"\\n\") if w.strip()]\n",
        "    df = generate_vocab_flashcards(vocab_list, language)\n",
        "    csv_path = \"anki_flashcards.csv\"\n",
        "\n",
        "    # Determine columns by language\n",
        "    if language.lower() in [\"japanese\", \"chinese\"]:\n",
        "        display_cols = [\"Grammar/Vocab\", \"Translation\", \"Reading\", \"Example Sentence\"]\n",
        "    else:\n",
        "        display_cols = [\"Grammar/Vocab\", \"Translation\", \"Example Sentence\"]\n",
        "        # If DataFrame has Reading, drop it for display and CSV\n",
        "        if \"Reading\" in df.columns:\n",
        "            df = df.drop(columns=[\"Reading\"])  # <-- fixed the split 'column\\ns' typo\n",
        "\n",
        "    df = df.reindex(columns=display_cols)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    return df, csv_path\n",
        "\n",
        "\n",
        "def grammar_chatbot_fn(\n",
        "    user_message,\n",
        "    history,\n",
        "    target_language=\"Spanish\",\n",
        "    native_language=\"English\",\n",
        "):\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    explanation, flashcard_data = generate_grammar_flashcard_and_explanation(\n",
        "        user_message=user_message,\n",
        "        target_language=target_language,\n",
        "        native_language=native_language,\n",
        "    )\n",
        "\n",
        "    # Save to CSV if JSON parsed correctly\n",
        "    if flashcard_data:\n",
        "        append_grammar_row(flashcard_data)\n",
        "\n",
        "    history.append((user_message, explanation))\n",
        "    return history\n",
        "\n",
        "\n",
        "def get_grammar_csv():\n",
        "    \"\"\"\n",
        "    Return the path to the grammar flashcards CSV for download in Gradio.\n",
        "    If it doesn't exist yet, create an empty file with just the header.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(GRAMMAR_CSV_PATH):\n",
        "        with open(GRAMMAR_CSV_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=GRAMMAR_COLUMNS)\n",
        "            writer.writeheader()\n",
        "    return GRAMMAR_CSV_PATH\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    # ---------- TAB 1: VOCAB FLASHCARDS ----------\n",
        "    with gr.Tab(\"Vocab Flashcards\"):\n",
        "        gr.Markdown(\"### TEN by10 – AI Vocab Flashcard Generator\")\n",
        "\n",
        "        vocab_text = gr.Textbox(\n",
        "            label=\"Enter vocab terms (one per line)\",\n",
        "            lines=8,\n",
        "            placeholder=\"e.g.\\nla organización\\nel animal\\nla tecnología\",\n",
        "        )\n",
        "\n",
        "        vocab_target_lang = gr.Radio(\n",
        "            [\"Spanish\", \"French\", \"Japanese\", \"Chinese\", \"English\"],\n",
        "            label=\"Target Language\",\n",
        "            value=\"Spanish\",\n",
        "        )\n",
        "\n",
        "        vocab_button = gr.Button(\"Generate Vocab Flashcards\")\n",
        "\n",
        "        vocab_table = gr.Dataframe(label=\"Generated Flashcards\")\n",
        "        vocab_csv = gr.File(label=\"Download CSV\")\n",
        "\n",
        "        vocab_button.click(\n",
        "            fn=run_flashcard_generator,\n",
        "            inputs=[vocab_text, vocab_target_lang],\n",
        "            outputs=[vocab_table, vocab_csv],\n",
        "        )\n",
        "\n",
        "# ---------- TAB 2: Grammar Chatbot ----------\n",
        "    with gr.Tab(\"Grammar Chatbot\"):\n",
        "        gr.Markdown(\n",
        "            \"### Grammar Tutor (builds grammar_flashcards.csv automatically)\"\n",
        "        )\n",
        "\n",
        "        target_lang_dropdown = gr.Dropdown(\n",
        "            [\"Spanish\", \"French\", \"Japanese\", \"Chinese\", \"English\"],\n",
        "            label=\"Target language\",\n",
        "            value=\"Spanish\",\n",
        "        )\n",
        "        native_lang_dropdown = gr.Dropdown(\n",
        "            [\"English\", \"Spanish\", \"French\", \"Japanese\", \"Chinese\"],\n",
        "            label=\"Your native language\",\n",
        "            value=\"English\",\n",
        "        )\n",
        "\n",
        "        grammar_chat = gr.Chatbot(label=\"Ask about grammar\")\n",
        "        grammar_input = gr.Textbox(\n",
        "            label=\"Type your grammar question\",\n",
        "            placeholder=(\n",
        "                \"Examples:\\n\"\n",
        "                \"- How do you conjugate 'tener' in Spanish?\\n\"\n",
        "                \"- How do you express 'to have' in Japanese (いる／ある／飼ってる／持ってる)?\"\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # NEW: buttons + file output\n",
        "        grammar_clear = gr.Button(\"Clear conversation\")\n",
        "        grammar_download = gr.Button(\"Download grammar CSV\")\n",
        "        grammar_file = gr.File(label=\"Grammar CSV\")\n",
        "\n",
        "        def grammar_wrapper(message, history, target_language, native_language):\n",
        "            return grammar_chatbot_fn(\n",
        "                user_message=message,\n",
        "                history=history,\n",
        "                target_language=target_language,\n",
        "                native_language=native_language,\n",
        "            )\n",
        "\n",
        "        grammar_input.submit(\n",
        "            fn=grammar_wrapper,\n",
        "            inputs=[grammar_input, grammar_chat, target_lang_dropdown, native_lang_dropdown],\n",
        "            outputs=grammar_chat,\n",
        "        )\n",
        "\n",
        "        grammar_clear.click(\n",
        "            fn=lambda: [],\n",
        "            inputs=None,\n",
        "            outputs=grammar_chat,\n",
        "        )\n",
        "\n",
        "        # NEW: wire download button to CSV-returning function\n",
        "        grammar_download.click(\n",
        "            fn=get_grammar_csv,\n",
        "            inputs=None,\n",
        "            outputs=grammar_file,\n",
        "        )\n",
        "\n",
        "\n",
        "  # ---------- TAB 3: CONVERSATION PARTNER (VOICE) ----------\n",
        "    with gr.Tab(\"Conversation Partner (Voice)\"):\n",
        "        gr.Markdown(\n",
        "            \"### Voice conversation partner\\n\"\n",
        "            \"Record your voice, get a spoken reply, and see the transcript.\"\n",
        "        )\n",
        "\n",
        "        conv_target_lang = gr.Dropdown(\n",
        "            [\"Spanish\", \"French\", \"Japanese\", \"Chinese\", \"English\"],\n",
        "            label=\"Target language\",\n",
        "            value=\"Spanish\",\n",
        "        )\n",
        "        conv_native_lang = gr.Dropdown(\n",
        "            [\"English\", \"Spanish\", \"French\", \"Japanese\", \"Chinese\"],\n",
        "            label=\"Your native language\",\n",
        "            value=\"English\",\n",
        "        )\n",
        "\n",
        "        voice_chat = gr.Chatbot(label=\"Conversation transcript\")\n",
        "\n",
        "        voice_input = gr.Audio(\n",
        "            sources=[\"microphone\"], # Changed 'source' to 'sources' and made it a list\n",
        "            type=\"filepath\",\n",
        "            label=\"Press to record, then release\",\n",
        "        )\n",
        "\n",
        "        voice_send = gr.Button(\"Send voice\")\n",
        "        voice_clear = gr.Button(\"Clear conversation\")\n",
        "\n",
        "        voice_reply_audio = gr.Audio(\n",
        "            label=\"Assistant reply (audio)\",\n",
        "            interactive=False,\n",
        "        )\n",
        "\n",
        "        def voice_wrapper(\n",
        "            audio_path,\n",
        "            history,\n",
        "            target_language,\n",
        "            native_language,\n",
        "        ):\n",
        "            return conversation_partner_voice_step(\n",
        "                audio_path=audio_path,\n",
        "                history=history,\n",
        "                target_language=target_language,\n",
        "                native_language=native_language,\n",
        "            )\n",
        "\n",
        "        voice_send.click(\n",
        "            fn=voice_wrapper,\n",
        "            inputs=[\n",
        "                voice_input,\n",
        "                voice_chat,\n",
        "                conv_target_lang,\n",
        "                conv_native_lang,\n",
        "            ],\n",
        "            outputs=[voice_chat, voice_reply_audio],\n",
        "        )\n",
        "\n",
        "        voice_clear.click(\n",
        "            fn=lambda: ([], None),\n",
        "            inputs=None,\n",
        "            outputs=[voice_chat, voice_reply_audio],\n",
        "        )\n",
        "\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
